{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffe3e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'measure_date_CET' to UTC and add a new column 'measure_date_utc'\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from pandas.api.types import is_datetime64_any_dtype\n",
    "\n",
    "# Prefer absolute path, fall back to workspace search\n",
    "fn = Path('/home/renga/Desktop/neoen_data/renga_work/data/sunnic/prices_upto_oct/da.csv')\n",
    "if not fn.exists():\n",
    "    fn = next(Path('.').rglob('pre_germany.csv'), None)\n",
    "    if fn is None:\n",
    "        raise FileNotFoundError('pre_germany.csv not found in workspace')\n",
    "\n",
    "print('Using file:', fn)\n",
    "df = pd.read_csv(fn)\n",
    "col = 'measure_date_CET'\n",
    "if col not in df.columns:\n",
    "    raise KeyError(f\"Column '{col}' not found in {fn}\")\n",
    "\n",
    "# Try vectorized parse first\n",
    "try:\n",
    "    times = pd.to_datetime(df[col].astype(str).str.strip(), errors='coerce', infer_datetime_format=True)\n",
    "    if not is_datetime64_any_dtype(times):\n",
    "        raise ValueError('Vectorized parse did not produce datetimelike dtype')\n",
    "\n",
    "    # Localize naive timestamps to Europe/Berlin and convert to UTC\n",
    "    try:\n",
    "        times_local = times.dt.tz_localize('Europe/Berlin', ambiguous='infer', nonexistent='shift_forward')\n",
    "    except Exception:\n",
    "        times_local = times.dt.tz_localize('Europe/Berlin', ambiguous='NaT', nonexistent='NaT')\n",
    "    times_utc = times_local.dt.tz_convert('UTC')\n",
    "    df['measure_date_utc'] = times_utc.astype(str)\n",
    "\n",
    "except Exception:\n",
    "    # Fallback: parse row-by-row using dateutil and pytz (more robust for messy inputs)\n",
    "    from dateutil import parser\n",
    "    try:\n",
    "        import pytz\n",
    "        berlin_tz = pytz.timezone('Europe/Berlin')\n",
    "        utc_tz = pytz.UTC\n",
    "    except Exception:\n",
    "        pytz = None\n",
    "        berlin_tz = None\n",
    "        import datetime\n",
    "        utc_tz = datetime.timezone.utc\n",
    "\n",
    "    def parse_row_to_utc(s):\n",
    "        if pd.isna(s):\n",
    "            return 'NaT'\n",
    "        s = str(s).strip()\n",
    "        if s == '':\n",
    "            return 'NaT'\n",
    "        try:\n",
    "            dt = parser.parse(s)\n",
    "        except Exception:\n",
    "            return 'NaT'\n",
    "        # If naive, localize to Europe/Berlin (best effort)\n",
    "        if dt.tzinfo is None:\n",
    "            if berlin_tz is not None:\n",
    "                try:\n",
    "                    dt = berlin_tz.localize(dt, is_dst=None)\n",
    "                except Exception:\n",
    "                    try:\n",
    "                        dt = berlin_tz.localize(dt, is_dst=False)\n",
    "                    except Exception:\n",
    "                        return 'NaT'\n",
    "            else:\n",
    "                # Best-effort: assume CET (UTC+1)\n",
    "                try:\n",
    "                    from datetime import timezone, timedelta\n",
    "                    dt = dt.replace(tzinfo=timezone(timedelta(hours=1)))\n",
    "                except Exception:\n",
    "                    return 'NaT'\n",
    "        # Convert to UTC\n",
    "        try:\n",
    "            if pytz is not None:\n",
    "                dt_utc = dt.astimezone(utc_tz)\n",
    "                return dt_utc.isoformat(sep=' ')\n",
    "            else:\n",
    "                dt_utc = dt.astimezone(utc_tz)\n",
    "                # isoformat gives timezone as +00:00\n",
    "                return dt_utc.isoformat(sep=' ')\n",
    "        except Exception:\n",
    "            return 'NaT'\n",
    "\n",
    "    df['measure_date_utc'] = df[col].apply(parse_row_to_utc)\n",
    "\n",
    "out = fn.with_name(fn.stem + '_utc' + fn.suffix)\n",
    "df.to_csv(out, index=False)\n",
    "print('Saved file with UTC column to', out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8931e088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec6d2ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2241e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsample DA prices (hourly -> 30-minute) and save a new CSV\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "fn = Path('/home/renga/Desktop/neoen_data/renga_work/data/sunnic/Longs_germany_utc.csv')\n",
    "if not fn.exists():\n",
    "    raise FileNotFoundError(f'{fn} not found')\n",
    "\n",
    "print('Loading', fn)\n",
    "df = pd.read_csv(fn)\n",
    "\n",
    "# detect date column\n",
    "candidates = ['measure_date', 'measure_date_utc', 'timestamp', 'time']\n",
    "date_col = None\n",
    "for c in candidates:\n",
    "    if c in df.columns:\n",
    "        date_col = c\n",
    "        break\n",
    "if date_col is None:\n",
    "    # fallback to any column that looks like date/time\n",
    "    for c in df.columns:\n",
    "        if 'date' in c.lower() or 'time' in c.lower():\n",
    "            date_col = c\n",
    "            break\n",
    "if date_col is None:\n",
    "    raise KeyError('No date/time column found in Longs file')\n",
    "\n",
    "print('Using date column:', date_col)\n",
    "\n",
    "# Parse to datetime\n",
    "ser = pd.to_datetime(df[date_col].astype(str).str.strip(), errors='coerce', infer_datetime_format=True)\n",
    "\n",
    "# Ensure timezone-aware UTC (file name suggests UTC). If tz-naive, localize to UTC.\n",
    "try:\n",
    "    if getattr(ser.dt, 'tz', None) is None:\n",
    "        ser = ser.dt.tz_localize('UTC')\n",
    "    else:\n",
    "        ser = ser.dt.tz_convert('UTC')\n",
    "except Exception:\n",
    "    # best-effort: convert dtype via pd.Timestamp\n",
    "    ser = pd.to_datetime(ser).dt.tz_localize('UTC')\n",
    "\n",
    "# Build a DataFrame indexed by the parsed times\n",
    "df2 = df.copy()\n",
    "# Drop original date column to avoid duplication after reset_index\n",
    "if date_col in df2.columns:\n",
    "    df2 = df2.drop(columns=[date_col])\n",
    "\n",
    "df2.index = ser\n",
    "# Sort index\n",
    "df2 = df2.sort_index()\n",
    "\n",
    "# Create full 30-minute index across the same span\n",
    "start = df2.index.min()\n",
    "end = df2.index.max()\n",
    "full_idx = pd.date_range(start=start, end=end, freq='30T', tz='UTC')\n",
    "\n",
    "# Reindex and forward-fill to propagate hourly value across quarter-hours\n",
    "df_30 = df2.reindex(full_idx)\n",
    "# Forward fill along rows so hourly values carry to quarter-hours\n",
    "df_30 = df_30.ffill()\n",
    "\n",
    "# Reset index back to column 'measure_date' with timezone info preserved\n",
    "df_30 = df_30.reset_index().rename(columns={'index': 'measure_date'})\n",
    "\n",
    "out = fn.with_name(fn.stem + '_30min' + fn.suffix)\n",
    "df_30.to_csv(out, index=False)\n",
    "print('Saved upsampled DA prices to', out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dc24a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Sunnic files on `measure_date` within the actual file's date range\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from pandas.api.types import is_datetime64_any_dtype\n",
    "\n",
    "base = Path('/home/renga/Desktop/neoen_data/renga_work/data/sunnic')\n",
    "files = {\n",
    "    'actual': base / 'actual_neuhardenberg_filled.csv',\n",
    "    'Longs': base / '/home/renga/Desktop/neoen_data/renga_work/data/sunnic/Longs_germany_utc_30min.csv',\n",
    "    'pre': base / 'pre_germany_utc.csv',\n",
    "    'imbalance': base / '/home/renga/Desktop/neoen_data/renga_work/data/sunnic/imb_volumes.csv',\n",
    "}\n",
    "\n",
    "def load_and_standardize(fn, prefer_col='measure_date'):\n",
    "    df = pd.read_csv(fn)\n",
    "    # Find a datetime-like column (prefer 'measure_date')\n",
    "    cand = None\n",
    "    for c in [prefer_col, 'measure_date_utc', 'measure_date_CET'] + list(df.columns):\n",
    "        if c in df.columns:\n",
    "            cand = c\n",
    "            break\n",
    "    if cand is None:\n",
    "        raise KeyError(f'No datetime-like column found in {fn}')\n",
    "\n",
    "    # Parse to datetime\n",
    "    ser = pd.to_datetime(df[cand], errors='coerce', infer_datetime_format=True)\n",
    "\n",
    "    # Ensure timezone-aware and in UTC\n",
    "    if is_datetime64_any_dtype(ser):\n",
    "        # If tz-naive, assume values labeled *_utc are already UTC; otherwise localize to UTC\n",
    "        try:\n",
    "            if getattr(ser.dt, 'tz', None) is None:\n",
    "                # If the original column name suggests UTC, localize as UTC, otherwise also localize to UTC\n",
    "                if 'utc' in cand.lower():\n",
    "                    ser = ser.dt.tz_localize('UTC')\n",
    "                else:\n",
    "                    ser = ser.dt.tz_localize('UTC')\n",
    "            else:\n",
    "                ser = ser.dt.tz_convert('UTC')\n",
    "        except Exception:\n",
    "            # best-effort: leave as is\n",
    "            pass\n",
    "    else:\n",
    "        # Fall back to per-row parse if vectorized failed\n",
    "        from dateutil import parser\n",
    "        from datetime import timezone, timedelta\n",
    "        def parse_row(x):\n",
    "            if pd.isna(x):\n",
    "                return pd.NaT\n",
    "            try:\n",
    "                dt = parser.parse(str(x).strip())\n",
    "            except Exception:\n",
    "                return pd.NaT\n",
    "            if dt.tzinfo is None:\n",
    "                # assume UTC when labelled _utc, else assume UTC as best-effort\n",
    "                return pd.Timestamp(dt).tz_localize('UTC')\n",
    "            return pd.Timestamp(dt).tz_convert('UTC')\n",
    "        ser = df[cand].apply(parse_row)\n",
    "\n",
    "    # Assign standardized column\n",
    "    df['measure_date'] = ser\n",
    "    return df\n",
    "\n",
    "# Load main file and determine date range\n",
    "main = load_and_standardize(files['actual'])\n",
    "min_date = main['measure_date'].min()\n",
    "max_date = main['measure_date'].max()\n",
    "print('Main date range:', min_date, '->', max_date)\n",
    "\n",
    "# Load and trim other files, then merge on 'measure_date'\n",
    "merged = main.copy()\n",
    "for key in ['Longs', 'pre', 'imbalance']:\n",
    "    f = files[key]\n",
    "    if not f.exists():\n",
    "        print(f'Warning: {f} not found, skipping')\n",
    "        continue\n",
    "    other = load_and_standardize(f)\n",
    "    # Restrict to main date range\n",
    "    other = other[(other['measure_date'] >= min_date) & (other['measure_date'] <= max_date)]\n",
    "    # Merge: keep all rows of main, bring columns from other (avoid duplicate measure_date)\n",
    "    # Drop duplicated columns in other that exist in merged except for 'measure_date'\n",
    "    cols_to_merge = [c for c in other.columns if c != 'measure_date']\n",
    "    print(f'Merging {key}: {other.shape[0]} rows, columns: {cols_to_merge[:5]}{\"...\" if len(cols_to_merge)>5 else \"\"}')\n",
    "    merged = merged.merge(other[['measure_date'] + cols_to_merge], on='measure_date', how='left', suffixes=(None, f'_{key}'))\n",
    "\n",
    "# Drop unwanted columns before saving (if present)\n",
    "cols_to_drop = [c for c in ['measure_date_CET', 'measure_date_CET_pre'] if c in merged.columns]\n",
    "if cols_to_drop:\n",
    "    merged = merged.drop(columns=cols_to_drop)\n",
    "    print('Dropped columns before saving:', cols_to_drop)\n",
    "\n",
    "out = base / 'combined_sunnic.csv'\n",
    "merged.to_csv(out, index=False)\n",
    "print('Saved merged file to', out)\n",
    "print('Merged shape:', merged.shape)\n",
    "merged.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c445c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'measure_date' is formatted as 'YYYY-MM-DD HH:MM:SS+0000' (UTC)\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "fn = Path('/home/renga/Desktop/neoen_data/renga_work/data/sunnic/combined_sunnic.csv')\n",
    "if not fn.exists():\n",
    "    raise FileNotFoundError(f'{fn} not found')\n",
    "\n",
    "df = pd.read_csv(fn)\n",
    "if 'measure_date' not in df.columns:\n",
    "    raise KeyError(\"Column 'measure_date' not found in the CSV\")\n",
    "\n",
    "# Parse to timezone-aware UTC datetimes and format without colon in tz offset\n",
    "df['measure_date'] = pd.to_datetime(df['measure_date'], errors='coerce', utc=True)\n",
    "df['measure_date'] = df['measure_date'].dt.strftime('%Y-%m-%d %H:%M:%S%z')\n",
    "\n",
    "out = fn.with_name(fn.stem + '_formatted' + fn.suffix)\n",
    "df.to_csv(out, index=False)\n",
    "print('Saved formatted file to', out)\n",
    "df['measure_date'].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a4ad1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check '/data/sunnic/imbalance_volumes.csv' for values in 'Imbalance Volume'\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "file_path = Path('/home/renga/Desktop/neoen_data/renga_work/data/sunnic/imbalance_volumes.csv')\n",
    "\n",
    "df_iv = pd.read_csv(file_path)\n",
    "col = 'Imbalance Volume'\n",
    "print('File:', file_path)\n",
    "print('Total rows:', len(df_iv))\n",
    "if col not in df_iv.columns:\n",
    "    print(f\"Column '{col}' not found. Available columns: {df_iv.columns.tolist()}\")\n",
    "else:\n",
    "    non_null = df_iv[col].notna().sum()\n",
    "    # count empty strings (after stripping) as empty\n",
    "    empty_strings = (df_iv[col].astype(str).str.strip() == '').sum()\n",
    "    # numeric zeros (coerce non-numeric to NaN first)\n",
    "    zeros = (pd.to_numeric(df_iv[col], errors='coerce') == 0).sum()\n",
    "    print(f\"Non-null '{col}':\", non_null)\n",
    "    print(f\"Empty-string rows in '{col}':\", empty_strings)\n",
    "    print(f\"Numeric zeros in '{col}':\", zeros)\n",
    "    # show a small sample of the column\n",
    "    print('\\nSample values:')\n",
    "    print(df_iv[col].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ae0373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge DA prices into combined_sunnic and fill missing Long\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "combined_fn = Path('/home/renga/Desktop/neoen_data/renga_work/data/sunnic/combined_sunnic_da_filled_Short.csv')\n",
    "da_fn = Path('/home/renga/Desktop/neoen_data/renga_work/data/sunnic/prices_upto_oct/pre.csv')\n",
    "\n",
    "if not combined_fn.exists():\n",
    "    raise FileNotFoundError(f'{combined_fn} not found')\n",
    "if not da_fn.exists():\n",
    "    raise FileNotFoundError(f'{da_fn} not found')\n",
    "\n",
    "print('Loading', combined_fn)\n",
    "combined = pd.read_csv(combined_fn)\n",
    "print('Loading', da_fn)\n",
    "da = pd.read_csv(da_fn)\n",
    "\n",
    "# Ensure measure_date column exists\n",
    "for df, name in [(combined, 'combined'), (da, 'da')]:\n",
    "    if 'measure_date' not in df.columns:\n",
    "        raise KeyError(f\"Column 'measure_date' not found in {name} file\")\n",
    "\n",
    "# Parse measure_date to timezone-aware UTC datetimes (best-effort)\n",
    "combined['measure_date'] = pd.to_datetime(combined['measure_date'].astype(str).str.strip(), errors='coerce', utc=True)\n",
    "da['measure_date'] = pd.to_datetime(da['measure_date'].astype(str).str.strip(), errors='coerce', utc=True)\n",
    "\n",
    "# Find DA price column in DA file (common names)\n",
    "price_candidates = ['Long', 'price', 'price_eur', 'value']\n",
    "Long_col = None\n",
    "for c in price_candidates:\n",
    "    if c in da.columns:\n",
    "        Long_col = c\n",
    "        break\n",
    "# If still not found, pick first numeric column that's not measure_date\n",
    "if Long_col is None:\n",
    "    numeric_cols = da.select_dtypes(include=['number']).columns.tolist()\n",
    "    numeric_cols = [c for c in numeric_cols if c != 'measure_date']\n",
    "    if numeric_cols:\n",
    "        Long_col = numeric_cols[0]\n",
    "    else:\n",
    "        raise KeyError('No suitable DA price column found in DA file')\n",
    "\n",
    "print('Using DA price column:', Long_col)\n",
    "# Build mapping from measure_date -> Long (ensure no duplicates; keep first)\n",
    "da_map = da.set_index('measure_date')[Long_col].sort_index()\n",
    "da_map = da_map[~da_map.index.duplicated(keep='first')]\n",
    "\n",
    "# Ensure combined has a Long column; if not, create it\n",
    "if 'Long' not in combined.columns:\n",
    "    combined['Long'] = pd.NA\n",
    "\n",
    "# Count missing before\n",
    "missing_before = combined['Long'].isna().sum()\n",
    "print('Missing Long before fill:', missing_before)\n",
    "\n",
    "# Fill missing by mapping using exact datetime matches\n",
    "# Use pandas Series.map which will align by Timestamp equality\n",
    "mapped = combined['measure_date'].map(da_map)\n",
    "combined['Long'] = combined['Long'].fillna(mapped)\n",
    "\n",
    "# If still missing, try merging on a rounded timestamp (30-min) as fallback\n",
    "still_missing = combined['Long'].isna().sum()\n",
    "if still_missing > 0:\n",
    "    print('Still missing after exact match:', still_missing)\n",
    "    # Try rounding to nearest 30 minutes both sides and attempt fill\n",
    "    da_map_30 = da.set_index(da['measure_date'].dt.round('30min'))[Long_col]\n",
    "    da_map_30 = da_map_30[~da_map_30.index.duplicated(keep='first')]\n",
    "    mapped30 = combined['measure_date'].dt.round('30min').map(da_map_30)\n",
    "    combined['Long'] = combined['Long'].fillna(mapped30)\n",
    "    still_missing = combined['Long'].isna().sum()\n",
    "    print('Still missing after 30-min fallback:', still_missing)\n",
    "\n",
    "# Save filled file\n",
    "out = combined_fn.with_name(combined_fn.stem + '_filled_Long' + combined_fn.suffix)\n",
    "combined.to_csv(out, index=False)\n",
    "print('Saved filled combined file to', out)\n",
    "print('Filled missing Long:', missing_before - combined['Long'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741051c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfb8992f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN counts per column:\n",
      "run_time                     0\n",
      "target_time                  0\n",
      "lead_hours                   0\n",
      "forecast_type                0\n",
      "actual_MWh                   0\n",
      "forecast_MWh                 0\n",
      "delta_MWh                    0\n",
      "da_price                     0\n",
      "revenue_EUR                  0\n",
      "applied_price_EUR_per_MWh    0\n",
      "penalty_EUR                  0\n",
      "net_revenue_EUR              0\n",
      "forecast_file                0\n",
      "dtype: int64\n",
      "\n",
      "NaN percentage per column:\n",
      "run_time                     0.0\n",
      "target_time                  0.0\n",
      "lead_hours                   0.0\n",
      "forecast_type                0.0\n",
      "actual_MWh                   0.0\n",
      "forecast_MWh                 0.0\n",
      "delta_MWh                    0.0\n",
      "da_price                     0.0\n",
      "revenue_EUR                  0.0\n",
      "applied_price_EUR_per_MWh    0.0\n",
      "penalty_EUR                  0.0\n",
      "net_revenue_EUR              0.0\n",
      "forecast_file                0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Count NaN values per column for the ledger CSV\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "fn = Path('/home/renga/Desktop/neoen_data/renga_work/combined_ledger_neoen_nan_to_0.csv')\n",
    "df = pd.read_csv(fn)\n",
    "nan_counts = df.isna().sum()\n",
    "print('NaN counts per column:')\n",
    "print(nan_counts)\n",
    "print('\\nNaN percentage per column:')\n",
    "pct = (nan_counts / len(df)) * 100\n",
    "print(pct.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "607b17fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved filled file to /home/renga/Desktop/neoen_data/renga_work/combined_ledger_neoen_nan_to_0.csv\n",
      "Remaining NaNs (total): 0\n"
     ]
    }
   ],
   "source": [
    "# Replace all NaN values with 0 and save a new CSV\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "fn = Path('/home/renga/Desktop/neoen_data/renga_work/combined_ledger_neoen.csv')\n",
    "if not fn.exists():\n",
    "    raise FileNotFoundError(f'{fn} not found')\n",
    "df = pd.read_csv(fn)\n",
    "# Fill NaNs with 0 for all columns\n",
    "df_filled = df.fillna(0)\n",
    "out = fn.with_name(fn.stem + '_nan_to_0' + fn.suffix)\n",
    "df_filled.to_csv(out, index=False)\n",
    "print('Saved filled file to', out)\n",
    "# Sanity check: total remaining NaNs (should be 0)\n",
    "print('Remaining NaNs (total):', int(df_filled.isna().sum().sum()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-2 (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
