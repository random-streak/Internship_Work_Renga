{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa0d4ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0f1ea79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Parameters ===\n",
    "DATA_PATH = \"/home/renga/Desktop/neoen_data/renga_work/data/sunnic/combined_sunnic_upto_oct.csv\"  \n",
    "TIME_COL = \"measure_date\"\n",
    "TARGET_COL = \"imbalance\"\n",
    "DA_PRICE_COL = \"da_price\"                        # optional exogenous\n",
    "\n",
    "# Model horizons & cadence\n",
    "HORIZON_HOURS  = 48   # <- change to alter forecast window (e.g., 24, 72, 96)\n",
    "STEP_MINUTES   = 15   # data granularity\n",
    "STRIDE_MINUTES = 60   # stride between rolling forecast origins during evaluation\n",
    "\n",
    "# History window to build lag/rolling features\n",
    "HISTORY_HOURS = 24    # <- change to 48/96/etc. to trade speed vs accuracy\n",
    "\n",
    "# Feature toggles\n",
    "USE_EXOG_DA = False   # <- set True to include lagged day-ahead price as exogenous\n",
    "\n",
    "# Train/test split cutoff (inclusive for train) — UTC as required\n",
    "TRAIN_CUTOFF_UTC = \"2025-08-31 00:00:00+00:00\"\n",
    "# Optional: specify a UTC start date for training. If None, training begins at the earliest available timestamp.\n",
    "# Example: TRAIN_START_UTC = \"2024-01-01 00:00:00+00:00\"\n",
    "TRAIN_START_UTC = \"2025-04-01 00:00:00+00:00\"\n",
    "\n",
    "# Optional strict 15-min reindexing. Set to None to skip.\n",
    "FREQ = \"15T\"\n",
    "\n",
    "# Evaluation speed control (set to None for full coverage; can be slower)\n",
    "MAX_ORIGINS = 200\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a9a9736f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running locally and missing dependencies, uncomment:\n",
    "# %pip install -q pandas numpy scikit-learn plotly ipywidgets\n",
    "# %pip install -q pyarrow  # faster CSV IO (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d382ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 5856 duplicated timestamps found in measure_date; aggregating by keeping last value for each timestamp\n",
      "Shape: (64524, 6)\n",
      "Columns: ['measure_date', 'prod', 'da_price', 'Long', 'Short', 'imbalance']\n",
      "Date range: 2024-01-01 07:00:00+00:00 -> 2025-11-03 09:45:00+00:00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>measure_date</th>\n",
       "      <th>prod</th>\n",
       "      <th>da_price</th>\n",
       "      <th>Long</th>\n",
       "      <th>Short</th>\n",
       "      <th>imbalance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-01 07:00:00+00:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-58.92</td>\n",
       "      <td>-58.92</td>\n",
       "      <td>69.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-01 07:15:00+00:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-57.68</td>\n",
       "      <td>-57.68</td>\n",
       "      <td>23.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-01 07:30:00+00:00</td>\n",
       "      <td>5.77</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-42.74</td>\n",
       "      <td>-42.74</td>\n",
       "      <td>43.89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               measure_date  prod  da_price   Long  Short  imbalance\n",
       "0 2024-01-01 07:00:00+00:00  0.00       0.0 -58.92 -58.92      69.34\n",
       "1 2024-01-01 07:15:00+00:00  0.00       0.0 -57.68 -57.68      23.61\n",
       "2 2024-01-01 07:30:00+00:00  5.77       0.0 -42.74 -42.74      43.89"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "assert TIME_COL in df.columns, f\"Missing '{TIME_COL}' in columns: {df.columns.tolist()}\"\n",
    "assert TARGET_COL in df.columns, f\"Missing '{TARGET_COL}' in columns: {df.columns.tolist()}\"\n",
    "\n",
    "# Parse datetime & sort\n",
    "df[TIME_COL] = pd.to_datetime(df[TIME_COL], utc=True, errors=\"coerce\")\n",
    "df = df.dropna(subset=[TIME_COL]).sort_values(TIME_COL).reset_index(drop=True)\n",
    "\n",
    "# Optional: align to strict 15-min grid (reindex).\n",
    "# When reindexing we must ensure the index has unique timestamps. If duplicates exist,\n",
    "# reindex will raise InvalidIndexError. We handle duplicates by aggregating (take last)\n",
    "# before reindexing so the operation succeeds.\n",
    "if FREQ is not None:\n",
    "    start, end = df[TIME_COL].min(), df[TIME_COL].max()\n",
    "    full_index = pd.date_range(start=start, end=end, freq=FREQ, tz=\"UTC\")\n",
    "    # Set the time column as index for reindexing\n",
    "    df = df.set_index(TIME_COL)\n",
    "    # If there are duplicated timestamps, aggregate them (keep last observation)\n",
    "    if df.index.duplicated().any():\n",
    "        n_dup = int(df.index.duplicated().sum())\n",
    "        print(f'Warning: {n_dup} duplicated timestamps found in {TIME_COL}; aggregating by keeping last value for each timestamp')\n",
    "        df = df[~df.index.duplicated(keep='last')]\n",
    "    # Reindex to full regular grid and reset index back to a column\n",
    "    df = df.reindex(full_index)\n",
    "    df.index.name = TIME_COL\n",
    "    df = df.reset_index()\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Columns:\", list(df.columns)[:20])\n",
    "print(\"Date range:\", df[TIME_COL].min(), \"->\", df[TIME_COL].max())\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4261d1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train period: 2025-04-01 00:00:00+00:00 -> 2025-08-31 00:00:00+00:00 rows: 14593\n",
      "Test  period: 2025-08-31 00:15:00+00:00 -> 2025-11-03 09:45:00+00:00 rows: 6183\n",
      "NaNs in target (train/test): 35 / 236\n"
     ]
    }
   ],
   "source": [
    "ts_cutoff = pd.Timestamp(TRAIN_CUTOFF_UTC)\n",
    "TRAIN_CUTOFF_UTC = ts_cutoff if ts_cutoff.tzinfo else ts_cutoff.tz_localize(\"UTC\")\n",
    "\n",
    "if TRAIN_START_UTC is not None:\n",
    "    ts_start = pd.Timestamp(TRAIN_START_UTC)\n",
    "    TRAIN_START_UTC = ts_start if ts_start.tzinfo else ts_start.tz_localize(\"UTC\")\n",
    "    train_df = df[(df[TIME_COL] >= TRAIN_START_UTC) & (df[TIME_COL] <= TRAIN_CUTOFF_UTC)].copy()\n",
    "else:\n",
    "    train_df = df[df[TIME_COL] <= TRAIN_CUTOFF_UTC].copy()\n",
    "\n",
    "test_df  = df[df[TIME_COL] >  TRAIN_CUTOFF_UTC].copy()\n",
    "\n",
    "print(\"Train period:\", train_df[TIME_COL].min(), \"->\", train_df[TIME_COL].max(), \"rows:\", len(train_df))\n",
    "print(\"Test  period:\",  test_df[TIME_COL].min(),  \"->\", test_df[TIME_COL].max(),  \"rows:\", len(test_df))\n",
    "print(\"NaNs in target (train/test):\", train_df[TARGET_COL].isna().sum(), \"/\", test_df[TARGET_COL].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee42c600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derived: steps/hour= 4 | N_LAGS= 96 | roll windows= [8, 24, 48, 96]\n"
     ]
    }
   ],
   "source": [
    "def build_features(\n",
    "    df_in: pd.DataFrame,\n",
    "    time_col: str,\n",
    "    target_col: str,\n",
    "    n_lags: int,\n",
    "    roll_windows: List[int],\n",
    "    use_exog_da: bool = False,\n",
    "    da_col: str = \"da_price_15min\",\n",
    "    add_time_feats: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert a time series into a supervised table for 1-step-ahead prediction.\n",
    "    Uses only past information (lags & rolling stats). Optionally include lagged DA price.\n",
    "    Optionally add time-of-day / day-of-week cyclical features when add_time_feats=True.\n",
    "    \"\"\"\n",
    "    df = df_in.copy().set_index(time_col).sort_index()\n",
    "    y = df[target_col].astype(float).copy()\n",
    "\n",
    "    feat = pd.DataFrame(index=df.index)\n",
    "\n",
    "    # Optional time features (hour of day, minute, day-of-week) and cyclical encoding\n",
    "    if add_time_feats:\n",
    "        ts = df.index\n",
    "        feat[\"hour\"] = ts.hour\n",
    "        feat[\"minute\"] = ts.minute\n",
    "        feat[\"dow\"] = ts.dayofweek\n",
    "        # cyclical encoding of time-of-day (better for circular patterns)\n",
    "        seconds_in_day = 24 * 3600\n",
    "        tod_seconds = ts.hour * 3600 + ts.minute * 60 + ts.second\n",
    "        feat[\"tod_sin\"] = np.sin(2 * np.pi * tod_seconds / seconds_in_day)\n",
    "        feat[\"tod_cos\"] = np.cos(2 * np.pi * tod_seconds / seconds_in_day)\n",
    "\n",
    "    # Lags of target\n",
    "    for lag in range(1, n_lags + 1):\n",
    "        feat[f\"lag_{lag}\"] = y.shift(lag)\n",
    "\n",
    "    # Rolling stats on target (shifted to avoid peeking)\n",
    "    for w in roll_windows:\n",
    "        feat[f\"roll_mean_{w}\"] = y.shift(1).rolling(w, min_periods=max(2, w//2)).mean()\n",
    "        feat[f\"roll_std_{w}\"]  = y.shift(1).rolling(w, min_periods=max(2, w//2)).std()\n",
    "\n",
    "    # Optional exogenous: lagged day-ahead price (safe to include if known before delivery)\n",
    "    if use_exog_da and (da_col in df.columns):\n",
    "        da = df[da_col].astype(float).copy()\n",
    "        for lag in range(0, min(96, n_lags+8)):  # ~24h context\n",
    "            feat[f\"da_lag_{lag}\"] = da.shift(lag)\n",
    "        for w in roll_windows:\n",
    "            feat[f\"da_roll_mean_{w}\"] = da.shift(1).rolling(w, min_periods=max(2, w//2)).mean()\n",
    "\n",
    "    # Target for 1-step ahead\n",
    "    feat[\"y_next\"] = y.shift(-1)\n",
    "\n",
    "    feat = feat.dropna().reset_index().rename(columns={time_col: \"ts\"})\n",
    "    return feat\n",
    "\n",
    "# Derive steps from cadence\n",
    "steps_per_hour = 60 // STEP_MINUTES\n",
    "N_LAGS = HISTORY_HOURS * steps_per_hour\n",
    "ROLL_WINDOWS = [2*steps_per_hour, 6*steps_per_hour, 12*steps_per_hour, 24*steps_per_hour]\n",
    "\n",
    "print(\"Derived: steps/hour=\", steps_per_hour, \"| N_LAGS=\", N_LAGS, \"| roll windows=\", ROLL_WINDOWS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b03e5638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data summary:\n",
      "  total rows in df: 64524\n",
      "  train rows: 14593\n",
      "  test rows:  6183\n",
      "  N_LAGS (history steps): 96\n",
      "  Steps per hour: 4\n",
      "  FREQ: 15T\n",
      "Train period (after applying TRAIN_START/END): 2025-04-01 00:00:00+00:00 -> 2025-08-31 00:00:00+00:00\n",
      "Test period: 2025-08-31 00:15:00+00:00 -> 2025-11-03 09:45:00+00:00\n",
      "NaNs in train key cols: {'measure_date': 0, 'imbalance': 35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_117882/4147035042.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feat[f\"roll_mean_{w}\"] = y.shift(1).rolling(w, min_periods=max(2, w//2)).mean()\n",
      "/tmp/ipykernel_117882/4147035042.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feat[f\"roll_std_{w}\"]  = y.shift(1).rolling(w, min_periods=max(2, w//2)).std()\n",
      "/tmp/ipykernel_117882/4147035042.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feat[f\"roll_mean_{w}\"] = y.shift(1).rolling(w, min_periods=max(2, w//2)).mean()\n",
      "/tmp/ipykernel_117882/4147035042.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feat[f\"roll_std_{w}\"]  = y.shift(1).rolling(w, min_periods=max(2, w//2)).std()\n",
      "/tmp/ipykernel_117882/4147035042.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feat[\"y_next\"] = y.shift(-1)\n",
      "/tmp/ipykernel_117882/4147035042.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feat[f\"roll_mean_{w}\"] = y.shift(1).rolling(w, min_periods=max(2, w//2)).mean()\n",
      "/tmp/ipykernel_117882/4147035042.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feat[f\"roll_std_{w}\"]  = y.shift(1).rolling(w, min_periods=max(2, w//2)).std()\n",
      "/tmp/ipykernel_117882/4147035042.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feat[f\"roll_mean_{w}\"] = y.shift(1).rolling(w, min_periods=max(2, w//2)).mean()\n",
      "/tmp/ipykernel_117882/4147035042.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feat[f\"roll_std_{w}\"]  = y.shift(1).rolling(w, min_periods=max(2, w//2)).std()\n",
      "/tmp/ipykernel_117882/4147035042.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feat[\"y_next\"] = y.shift(-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shapes -> X_train: (14188, 104) X_test: (5850, 104)\n",
      "1-step ahead Test MAE:  49.91\n",
      "Train supervised shape: (14188, 104) | Test supervised shape: (5850, 104)\n"
     ]
    }
   ],
   "source": [
    "# Diagnostic checks + Build features\n",
    "needed_cols = [TIME_COL, TARGET_COL] + ([DA_PRICE_COL] if USE_EXOG_DA else [])\n",
    "\n",
    "print('Data summary:')\n",
    "print('  total rows in df:', len(df))\n",
    "print('  train rows:', len(train_df))\n",
    "print('  test rows: ', len(test_df))\n",
    "print('  N_LAGS (history steps):', N_LAGS)\n",
    "print('  Steps per hour:', steps_per_hour)\n",
    "print('  FREQ:', FREQ)\n",
    "\n",
    "# Quick sanity checks\n",
    "if len(train_df) <= N_LAGS:\n",
    "    raise ValueError(\n",
    "        f\"Insufficient training rows for N_LAGS={N_LAGS}. \\n\"\n",
    "        f\"Train rows={len(train_df)}. Increase training window, lower HISTORY_HOURS or STEP_MINUTES, \"\n",
    "        \"or set TRAIN_START_UTC earlier.\"\n",
    "    )\n",
    "\n",
    "# Show first/last timestamps in train_df to help debugging\n",
    "print('Train period (after applying TRAIN_START/END):', train_df[TIME_COL].min(), '->', train_df[TIME_COL].max())\n",
    "print('Test period:', test_df[TIME_COL].min(), '->', test_df[TIME_COL].max())\n",
    "\n",
    "# Check for NaNs in key columns\n",
    "nan_summary = {c: int(train_df[c].isna().sum()) for c in needed_cols if c in train_df.columns}\n",
    "print('NaNs in train key cols:', nan_summary)\n",
    "\n",
    "# Build features\n",
    "train_feat = build_features(\n",
    "    train_df[needed_cols], time_col=TIME_COL, target_col=TARGET_COL,\n",
    "    n_lags=N_LAGS, roll_windows=ROLL_WINDOWS, use_exog_da=USE_EXOG_DA, da_col=DA_PRICE_COL\n",
    ")\n",
    "\n",
    "test_feat = build_features(\n",
    "    test_df[needed_cols], time_col=TIME_COL, target_col=TARGET_COL,\n",
    "    n_lags=N_LAGS, roll_windows=ROLL_WINDOWS, use_exog_da=USE_EXOG_DA, da_col=DA_PRICE_COL\n",
    ")\n",
    "\n",
    "# If features are empty, provide actionable diagnostics\n",
    "if train_feat.empty:\n",
    "    print('\\n=== ERROR: train_feat is empty after feature construction ===')\n",
    "    print('  - Check that train_df has enough chronological rows (>= N_LAGS + a few).')\n",
    "    print('  - Check for NaNs in target column that may reduce available rows after dropna:')\n",
    "    print(train_df[TARGET_COL].describe())\n",
    "    print('  - Suggested fixes: set TRAIN_START_UTC earlier, reduce HISTORY_HOURS, or set FREQ=None to avoid aggressive reindexing.')\n",
    "    raise ValueError('train_feat is empty; cannot train model. See diagnostics printed above.')\n",
    "\n",
    "if test_feat.empty:\n",
    "    print('\\nWarning: test_feat is empty after feature construction — no evaluation will be performed.')\n",
    "\n",
    "feature_cols = [c for c in train_feat.columns if c not in (\"ts\",\"y_next\")]\n",
    "X_train = train_feat[feature_cols].values\n",
    "y_train = train_feat[\"y_next\"].values\n",
    "X_test  = test_feat[feature_cols].values\n",
    "y_test  = test_feat[\"y_next\"].values\n",
    "\n",
    "print('Feature matrix shapes -> X_train:', X_train.shape, 'X_test:', X_test.shape)\n",
    "\n",
    "model = Pipeline([\n",
    "    (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "    (\"reg\", Ridge(alpha=1.0, random_state=RANDOM_STATE))\n",
    "])\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_1 = model.predict(X_test)\n",
    "mae_1 = mean_absolute_error(y_test, y_pred_1)\n",
    "# rmse_1 = mean_squared_error(y_test, y_pred_1, squared=False)\n",
    "print(f\"1-step ahead Test MAE:  {mae_1:.2f}\")\n",
    "# print(f\"1-step ahead Test RMSE: {rmse_1:.2f}\")\n",
    "print(\"Train supervised shape:\", X_train.shape, \"| Test supervised shape:\", X_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c989815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Origins evaluated: 200 | Horizon steps: 192 | Rows: 38400\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h</th>\n",
       "      <th>MAE</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>48.919864</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>64.615697</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>72.239515</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>78.948583</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>84.894658</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   h        MAE  Count\n",
       "0  1  48.919864  200.0\n",
       "1  2  64.615697  200.0\n",
       "2  3  72.239515  200.0\n",
       "3  4  78.948583  200.0\n",
       "4  5  84.894658  200.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature order for recursive forecasting must match training's feature_cols\n",
    "FEAT_ORDER = feature_cols.copy()\n",
    "\n",
    "def xrow_from_buffer(buf: np.ndarray, n_lags: int, roll_windows: List[int]) -> np.ndarray:\n",
    "    \"\"\"Build a single feature row in FEAT_ORDER from the history buffer.\"\"\"\n",
    "    parts = {}\n",
    "    # Lags: lag_1..lag_n (most recent first)\n",
    "    lags_vals = buf[-n_lags:][::-1]\n",
    "    for i, v in enumerate(lags_vals, start=1):\n",
    "        parts[f\"lag_{i}\"] = v\n",
    "    # Rolling stats (exclude the most recent point)\n",
    "    for w in roll_windows:\n",
    "        if len(buf) < w + 1:\n",
    "            use = buf[:-1] if len(buf) > 1 else buf\n",
    "        else:\n",
    "            use = buf[-(w+1):-1]\n",
    "        parts[f\"roll_mean_{w}\"] = float(np.mean(use)) if len(use) else float(buf[-1])\n",
    "        parts[f\"roll_std_{w}\"]  = float(np.std(use, ddof=1)) if len(use) > 1 else 0.0\n",
    "\n",
    "    # If DA exogenous was enabled, fill any missing da_* keys with zeros to match shape\n",
    "    if USE_EXOG_DA:\n",
    "        for k in FEAT_ORDER:\n",
    "            if k not in parts:\n",
    "                parts[k] = 0.0\n",
    "\n",
    "    # Order features\n",
    "    return np.array([parts[k] for k in FEAT_ORDER], dtype=float).reshape(1, -1)\n",
    "\n",
    "def recursive_forecast_fast(history_values: np.ndarray, H: int, n_lags: int, roll_windows: List[int]) -> np.ndarray:\n",
    "    \"\"\"Recursive forecast using the trained 1-step model.\"\"\"\n",
    "    buf = np.asarray(history_values, dtype=float).copy()\n",
    "    preds = np.empty(H, dtype=float)\n",
    "    for t in range(H):\n",
    "        x_row = xrow_from_buffer(buf, n_lags=n_lags, roll_windows=roll_windows)\n",
    "        preds[t] = float(model.predict(x_row)[0])\n",
    "        buf = np.append(buf, preds[t])\n",
    "    return preds\n",
    "\n",
    "# Build continuous series across train+test for history access\n",
    "full = pd.concat([train_df[[TIME_COL, TARGET_COL]], test_df[[TIME_COL, TARGET_COL]]]).sort_values(TIME_COL).reset_index(drop=True)\n",
    "full = full.dropna(subset=[TARGET_COL])\n",
    "# Make a UTC-aware index for robust lookups\n",
    "full_idx = pd.Index(pd.to_datetime(full[TIME_COL], utc=True))\n",
    "# Precompute integer nanosecond values for a fallback exact-match lookup\n",
    "full_ns = full[TIME_COL].astype('int64').values\n",
    "\n",
    "H = int(HORIZON_HOURS * (60 // STEP_MINUTES))\n",
    "stride = max(1, STRIDE_MINUTES // STEP_MINUTES)\n",
    "\n",
    "# Rolling origins within the test region\n",
    "origins = []\n",
    "start_origin = test_df[TIME_COL].min()\n",
    "for i in range(0, len(test_df), stride):\n",
    "    ts = test_df.iloc[i][TIME_COL]\n",
    "    if ts >= start_origin:\n",
    "        origins.append(ts)\n",
    "if MAX_ORIGINS is not None:\n",
    "    origins = origins[:MAX_ORIGINS]\n",
    "\n",
    "rows = []\n",
    "y_map = dict(zip(full[TIME_COL].astype(np.int64), full[TARGET_COL]))\n",
    "\n",
    "for origin in origins:\n",
    "    # Normalize origin to a UTC-aware Timestamp to avoid naive/aware mismatches\n",
    "    try:\n",
    "        origin_ts = pd.Timestamp(origin)\n",
    "        if origin_ts.tzinfo is None:\n",
    "            origin_ts = origin_ts.tz_localize('UTC')\n",
    "        else:\n",
    "            origin_ts = origin_ts.tz_convert('UTC')\n",
    "    except Exception:\n",
    "        # If Timestamp conversion fails, skip this origin\n",
    "        continue\n",
    "\n",
    "    # Try a safe indexer lookup first (returns -1 when not found)\n",
    "    pos = full_idx.get_indexer([origin_ts])[0]\n",
    "    if pos == -1:\n",
    "        # Fallback: match integer nanosecond representation (exact match)\n",
    "        try:\n",
    "            origin_ns = int(origin_ts.value)\n",
    "            pos_arr = np.where(full_ns == origin_ns)[0]\n",
    "            if pos_arr.size == 0:\n",
    "                continue\n",
    "            pos = int(pos_arr[0])\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    if pos < N_LAGS:\n",
    "        continue  # not enough history\n",
    "\n",
    "    history = full.iloc[:pos][TARGET_COL].values[-N_LAGS:]\n",
    "    preds = recursive_forecast_fast(history_values=history, H=H, n_lags=N_LAGS, roll_windows=ROLL_WINDOWS)\n",
    "\n",
    "    for h in range(1, H+1):\n",
    "        target_ts = origin + pd.Timedelta(minutes=STEP_MINUTES*h)\n",
    "        key = int(target_ts.value)\n",
    "        y_true = y_map.get(key, np.nan)\n",
    "        rows.append({\n",
    "            \"origin_ts\": origin,\n",
    "            \"h\": h,\n",
    "            \"target_ts\": target_ts,\n",
    "            \"y_pred\": preds[h-1],\n",
    "            \"y_true\": y_true\n",
    "        })\n",
    "\n",
    "rolling_forecasts = pd.DataFrame(rows)\n",
    "valid = rolling_forecasts.dropna(subset=[\"y_true\"])\n",
    "metrics = valid.groupby(\"h\").apply(lambda g: pd.Series({\n",
    "    \"MAE\":  mean_absolute_error(g[\"y_true\"], g[\"y_pred\"]),\n",
    "    # \"RMSE\": mean_squared_error(g[\"y_true\"], g[\"y_pred\"], squared=False),\n",
    "    \"Count\": len(g)\n",
    "})).reset_index()\n",
    "\n",
    "print(\"Origins evaluated:\", len(origins), \"| Horizon steps:\", H, \"| Rows:\", len(rolling_forecasts))\n",
    "metrics.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "103206fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quick test (time features) 1-step Test MAE: 51.88 | rows train/test: (4903, 109)/(903, 109)\n",
      "Original 1-step Test MAE (no time feats): 49.91\n",
      "MAE change (orig - time_feats) = -1.976  (positive => improvement)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_117882/4147035042.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feat[f\"lag_{lag}\"] = y.shift(lag)\n",
      "/tmp/ipykernel_117882/4147035042.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feat[f\"roll_mean_{w}\"] = y.shift(1).rolling(w, min_periods=max(2, w//2)).mean()\n",
      "/tmp/ipykernel_117882/4147035042.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feat[f\"roll_std_{w}\"]  = y.shift(1).rolling(w, min_periods=max(2, w//2)).std()\n",
      "/tmp/ipykernel_117882/4147035042.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feat[f\"roll_mean_{w}\"] = y.shift(1).rolling(w, min_periods=max(2, w//2)).mean()\n",
      "/tmp/ipykernel_117882/4147035042.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feat[f\"roll_std_{w}\"]  = y.shift(1).rolling(w, min_periods=max(2, w//2)).std()\n",
      "/tmp/ipykernel_117882/4147035042.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feat[f\"roll_mean_{w}\"] = y.shift(1).rolling(w, min_periods=max(2, w//2)).mean()\n",
      "/tmp/ipykernel_117882/4147035042.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feat[f\"roll_std_{w}\"]  = y.shift(1).rolling(w, min_periods=max(2, w//2)).std()\n",
      "/tmp/ipykernel_117882/4147035042.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feat[f\"roll_mean_{w}\"] = y.shift(1).rolling(w, min_periods=max(2, w//2)).mean()\n",
      "/tmp/ipykernel_117882/4147035042.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feat[f\"roll_std_{w}\"]  = y.shift(1).rolling(w, min_periods=max(2, w//2)).std()\n",
      "/tmp/ipykernel_117882/4147035042.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feat[\"y_next\"] = y.shift(-1)\n",
      "/tmp/ipykernel_117882/4147035042.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feat[f\"lag_{lag}\"] = y.shift(lag)\n",
      "/tmp/ipykernel_117882/4147035042.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feat[f\"roll_mean_{w}\"] = y.shift(1).rolling(w, min_periods=max(2, w//2)).mean()\n",
      "/tmp/ipykernel_117882/4147035042.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feat[f\"roll_std_{w}\"]  = y.shift(1).rolling(w, min_periods=max(2, w//2)).std()\n",
      "/tmp/ipykernel_117882/4147035042.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feat[f\"roll_mean_{w}\"] = y.shift(1).rolling(w, min_periods=max(2, w//2)).mean()\n",
      "/tmp/ipykernel_117882/4147035042.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feat[f\"roll_std_{w}\"]  = y.shift(1).rolling(w, min_periods=max(2, w//2)).std()\n",
      "/tmp/ipykernel_117882/4147035042.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feat[f\"roll_mean_{w}\"] = y.shift(1).rolling(w, min_periods=max(2, w//2)).mean()\n",
      "/tmp/ipykernel_117882/4147035042.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feat[f\"roll_std_{w}\"]  = y.shift(1).rolling(w, min_periods=max(2, w//2)).std()\n",
      "/tmp/ipykernel_117882/4147035042.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feat[f\"roll_mean_{w}\"] = y.shift(1).rolling(w, min_periods=max(2, w//2)).mean()\n",
      "/tmp/ipykernel_117882/4147035042.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feat[f\"roll_std_{w}\"]  = y.shift(1).rolling(w, min_periods=max(2, w//2)).std()\n",
      "/tmp/ipykernel_117882/4147035042.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  feat[\"y_next\"] = y.shift(-1)\n"
     ]
    }
   ],
   "source": [
    "# Quick experiment: train & evaluate a model including time-of-day / day-of-week features on a small subset\n",
    "Q_N_LAGS = min(N_LAGS, 96)  # use smaller lag for a fast check\n",
    "N_TRAIN = 5000\n",
    "N_TEST = 1000\n",
    "train_sub = train_df.tail(N_TRAIN) if len(train_df) > N_TRAIN else train_df\n",
    "test_sub  = test_df.head(N_TEST) if len(test_df) > N_TEST else test_df\n",
    "needed_cols = [TIME_COL, TARGET_COL] + ([DA_PRICE_COL] if USE_EXOG_DA else [])\n",
    "train_feat_tf = build_features(train_sub[needed_cols], time_col=TIME_COL, target_col=TARGET_COL, n_lags=Q_N_LAGS, roll_windows=ROLL_WINDOWS, use_exog_da=USE_EXOG_DA, da_col=DA_PRICE_COL, add_time_feats=True)\n",
    "test_feat_tf  = build_features(test_sub[needed_cols],  time_col=TIME_COL, target_col=TARGET_COL, n_lags=Q_N_LAGS, roll_windows=ROLL_WINDOWS, use_exog_da=USE_EXOG_DA, da_col=DA_PRICE_COL, add_time_feats=True)\n",
    "if train_feat_tf.empty or test_feat_tf.empty:\n",
    "    print('Not enough rows to build time-feature test with Q_N_LAGS=', Q_N_LAGS)\n",
    "else:\n",
    "    feat_tf_cols = [c for c in train_feat_tf.columns if c not in ('ts','y_next')]\n",
    "    X_train_tf = train_feat_tf[feat_tf_cols].values\n",
    "    y_train_tf = train_feat_tf['y_next'].values\n",
    "    X_test_tf  = test_feat_tf[feat_tf_cols].values\n",
    "    y_test_tf  = test_feat_tf['y_next'].values\n",
    "    model_tf = Pipeline([('scaler', StandardScaler()), ('reg', Ridge(alpha=1.0, random_state=RANDOM_STATE))])\n",
    "    model_tf.fit(X_train_tf, y_train_tf)\n",
    "    y_pred_tf = model_tf.predict(X_test_tf)\n",
    "    mae_tf = mean_absolute_error(y_test_tf, y_pred_tf)\n",
    "    print(f'Quick test (time features) 1-step Test MAE: {mae_tf:.2f} | rows train/test: {X_train_tf.shape}/{X_test_tf.shape}')\n",
    "    print(f'Original 1-step Test MAE (no time feats): {mae_1:.2f}')\n",
    "    delta = mae_1 - mae_tf\n",
    "    print(f'MAE change (orig - time_feats) = {delta:.3f}  (positive => improvement)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ca763c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-step sign accuracy: 80.89%   (n=5850)\n",
      "Rolling forecasts overall sign accuracy: 56.02%   (rows=38400)\n",
      "Per-horizon sign accuracy (first 10 horizons):\n",
      "    h  SignAcc  Count\n",
      "0   1     84.0  200.0\n",
      "1   2     79.0  200.0\n",
      "2   3     75.5  200.0\n",
      "3   4     70.0  200.0\n",
      "4   5     62.5  200.0\n",
      "5   6     57.5  200.0\n",
      "6   7     56.5  200.0\n",
      "7   8     54.5  200.0\n",
      "8   9     53.5  200.0\n",
      "9  10     50.0  200.0\n",
      "Per-horizon sign accuracy (first 10 horizons):\n",
      "    h  SignAcc  Count\n",
      "0   1     84.0  200.0\n",
      "1   2     79.0  200.0\n",
      "2   3     75.5  200.0\n",
      "3   4     70.0  200.0\n",
      "4   5     62.5  200.0\n",
      "5   6     57.5  200.0\n",
      "6   7     56.5  200.0\n",
      "7   8     54.5  200.0\n",
      "8   9     53.5  200.0\n",
      "9  10     50.0  200.0\n"
     ]
    }
   ],
   "source": [
    "# Sign-only accuracy: compare sign(pred) == sign(true) for 1-step and rolling forecasts\n",
    "import pandas as _pd\n",
    "# 1-step test set accuracy (uses y_pred_1 and y_test from above)\n",
    "try:\n",
    "    pred_sign_1 = np.sign(y_pred_1)\n",
    "    true_sign_1 = np.sign(y_test)\n",
    "    sign_acc_1 = (pred_sign_1 == true_sign_1).mean() * 100\n",
    "    print(f'1-step sign accuracy: {sign_acc_1:.2f}%   (n={len(true_sign_1)})')\n",
    "except NameError:\n",
    "    print('1-step variables y_pred_1 / y_test not found in scope; run the 1-step training cell first')\n",
    "\n",
    "# Rolling multi-step sign accuracy per horizon (requires rolling_forecasts / valid)\n",
    "if 'rolling_forecasts' in globals():\n",
    "    rf = rolling_forecasts.copy()\n",
    "    rf = rf.dropna(subset=['y_true'])\n",
    "    if rf.empty:\n",
    "        print('No valid rolling forecasts with y_true available to compute sign accuracy.')\n",
    "    else:\n",
    "        rf['pred_sign'] = np.sign(rf['y_pred'].values)\n",
    "        rf['true_sign'] = np.sign(rf['y_true'].values)\n",
    "        overall = (rf['pred_sign'] == rf['true_sign']).mean() * 100\n",
    "        print(f'Rolling forecasts overall sign accuracy: {overall:.2f}%   (rows={len(rf)})')\n",
    "        per_h = rf.groupby('h').apply(lambda g: _pd.Series({'SignAcc': (g['pred_sign']==g['true_sign']).mean()*100, 'Count': len(g)})).reset_index()\n",
    "        print('Per-horizon sign accuracy (first 10 horizons):')\n",
    "        print(per_h.head(10))\n",
    "else:\n",
    "    print('rolling_forecasts not found in scope; run the rolling forecast evaluation cell first')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "830b3637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1-step sign forecasts to sunnic/outputs_same_trial2/one_step_sign_forecasts_2.csv | rows=5850\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>origin_ts</th>\n",
       "      <th>h</th>\n",
       "      <th>target_ts</th>\n",
       "      <th>y_true</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>pred_state</th>\n",
       "      <th>correctness</th>\n",
       "      <th>correctness_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-09-01 00:15:00+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-09-01 00:30:00+00:00</td>\n",
       "      <td>75.18</td>\n",
       "      <td>71.736998</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-09-01 00:30:00+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-09-01 00:45:00+00:00</td>\n",
       "      <td>63.22</td>\n",
       "      <td>29.557266</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-09-01 00:45:00+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-09-01 01:00:00+00:00</td>\n",
       "      <td>125.62</td>\n",
       "      <td>64.309118</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-09-01 01:00:00+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-09-01 01:15:00+00:00</td>\n",
       "      <td>175.04</td>\n",
       "      <td>49.215879</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-09-01 01:15:00+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-09-01 01:30:00+00:00</td>\n",
       "      <td>105.60</td>\n",
       "      <td>96.854199</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  origin_ts  h                 target_ts  y_true     y_pred  \\\n",
       "0 2025-09-01 00:15:00+00:00  1 2025-09-01 00:30:00+00:00   75.18  71.736998   \n",
       "1 2025-09-01 00:30:00+00:00  1 2025-09-01 00:45:00+00:00   63.22  29.557266   \n",
       "2 2025-09-01 00:45:00+00:00  1 2025-09-01 01:00:00+00:00  125.62  64.309118   \n",
       "3 2025-09-01 01:00:00+00:00  1 2025-09-01 01:15:00+00:00  175.04  49.215879   \n",
       "4 2025-09-01 01:15:00+00:00  1 2025-09-01 01:30:00+00:00  105.60  96.854199   \n",
       "\n",
       "   pred_state  correctness correctness_label  \n",
       "0           1         True           correct  \n",
       "1           1         True           correct  \n",
       "2           1         True           correct  \n",
       "3           1         True           correct  \n",
       "4           1         True           correct  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save 1-step sign forecast results to CSV\n",
    "# Columns: origin_ts,h,target_ts,y_true,y_pred,pred_state,correctness,correctness_label\n",
    "from pathlib import Path\n",
    "try:\n",
    "    out_dir = Path(\"sunnic/outputs_same_trial2\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # test_feat contains 'ts' and 'y_next' used earlier to build X_test / y_test\n",
    "    origin_ts = test_feat['ts']\n",
    "    df_1 = pd.DataFrame({\n",
    "        'origin_ts': origin_ts,\n",
    "        'h': 1,\n",
    "        'target_ts': origin_ts + pd.Timedelta(minutes=STEP_MINUTES),\n",
    "        'y_true': test_feat['y_next'].values,\n",
    "        'y_pred': y_pred_1,\n",
    "    })\n",
    "\n",
    "    # predicted state as sign (-1, 0, 1)\n",
    "    df_1['pred_state'] = np.sign(df_1['y_pred']).astype(int)\n",
    "    # correctness: sign(pred) == sign(true)\n",
    "    df_1['correctness'] = (np.sign(df_1['y_pred']) == np.sign(df_1['y_true']))\n",
    "    df_1['correctness_label'] = df_1['correctness'].map({True: 'correct', False: 'incorrect'})\n",
    "\n",
    "    out_path = out_dir / 'one_step_sign_forecasts_2.csv'\n",
    "    df_1.to_csv(out_path, index=False)\n",
    "    print(f\"Saved 1-step sign forecasts to {out_path} | rows={len(df_1)}\")\n",
    "    try:\n",
    "        display(df_1.head())\n",
    "    except NameError:\n",
    "        # display may not be available in some runtimes; silently continue\n",
    "        pass\n",
    "except Exception as e:\n",
    "    print('Could not save 1-step sign forecasts:', e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-3 (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
